{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('pickels/180k_apparel_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>183138</td>\n",
       "      <td>182987</td>\n",
       "      <td>64956</td>\n",
       "      <td>183138</td>\n",
       "      <td>183138</td>\n",
       "      <td>183138</td>\n",
       "      <td>28395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>183138</td>\n",
       "      <td>10577</td>\n",
       "      <td>7380</td>\n",
       "      <td>72</td>\n",
       "      <td>170782</td>\n",
       "      <td>175985</td>\n",
       "      <td>3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>B01D5VRTJ0</td>\n",
       "      <td>Zago</td>\n",
       "      <td>Black</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>Nakoda Cotton Self Print Straight Kurti For Women</td>\n",
       "      <td>$19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>13207</td>\n",
       "      <td>167794</td>\n",
       "      <td>23</td>\n",
       "      <td>77</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin   brand  color product_type_name  \\\n",
       "count       183138  182987  64956            183138   \n",
       "unique      183138   10577   7380                72   \n",
       "top     B01D5VRTJ0    Zago  Black             SHIRT   \n",
       "freq             1     223  13207            167794   \n",
       "\n",
       "                                         medium_image_url  \\\n",
       "count                                              183138   \n",
       "unique                                             170782   \n",
       "top     https://images-na.ssl-images-amazon.com/images...   \n",
       "freq                                                   23   \n",
       "\n",
       "                                                    title formatted_price  \n",
       "count                                              183138           28395  \n",
       "unique                                             175985            3135  \n",
       "top     Nakoda Cotton Self Print Straight Kurti For Women          $19.99  \n",
       "freq                                                   77             945  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating Color=NULL : 64956\n",
      "Number of data points After eliminating Brand=NULL : 64843\n",
      "Number of data points After eliminating Product_Type_Name=NULL : 64843\n"
     ]
    }
   ],
   "source": [
    "# consider products which have price information\n",
    "# data['Column_name'].isnull() => gives the information\n",
    "# about the dataframe row's which have null values price == None|Null\n",
    "data = data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating Color=NULL :', data.shape[0])\n",
    "data = data.loc[~data['brand'].isnull()]\n",
    "print('Number of data points After eliminating Brand=NULL :', data.shape[0])\n",
    "data = data.loc[~data['product_type_name'].isnull()]\n",
    "print('Number of data points After eliminating Product_Type_Name=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/64k_apparel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates From Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of products with short description: 63263\n"
     ]
    }
   ],
   "source": [
    "data_sorted = data[data['title'].apply(lambda x: len(x.split())>4)]\n",
    "print(\"After removal of products with short description:\", data_sorted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\projects\\project reference\\apparel-recommendation-system\\version 2.0\\arsv2\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27547</th>\n",
       "      <td>B073W7P8KK</td>\n",
       "      <td>Nation LTD</td>\n",
       "      <td>Blue</td>\n",
       "      <td>DRESS</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>❀Nation Women Stripe Blouse Long Sleeve Shirt ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31277</th>\n",
       "      <td>B01M0PWMZ8</td>\n",
       "      <td>Anglin</td>\n",
       "      <td>White</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>✽ANGLIN✽ Women Striped Floral Long Sleeve Roun...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30453</th>\n",
       "      <td>B01M02GWRG</td>\n",
       "      <td>Anglin</td>\n",
       "      <td>White</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>✽ANGLIN✽ Women Striped Floral Long Sleeve Roun...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32485</th>\n",
       "      <td>B01N0ADXM0</td>\n",
       "      <td>Anglin</td>\n",
       "      <td>Red</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>✽ANGLIN✽ Women Fashion Stripe Dress Round Coll...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26767</th>\n",
       "      <td>B01MTQAU86</td>\n",
       "      <td>Anglin</td>\n",
       "      <td>Black</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>✽ANGLIN✽ Women Autumn Winter Christmas Printin...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin       brand  color product_type_name  \\\n",
       "27547  B073W7P8KK  Nation LTD   Blue             DRESS   \n",
       "31277  B01M0PWMZ8      Anglin  White             SHIRT   \n",
       "30453  B01M02GWRG      Anglin  White             SHIRT   \n",
       "32485  B01N0ADXM0      Anglin    Red             SHIRT   \n",
       "26767  B01MTQAU86      Anglin  Black             SHIRT   \n",
       "\n",
       "                                        medium_image_url  \\\n",
       "27547  https://images-na.ssl-images-amazon.com/images...   \n",
       "31277  https://images-na.ssl-images-amazon.com/images...   \n",
       "30453  https://images-na.ssl-images-amazon.com/images...   \n",
       "32485  https://images-na.ssl-images-amazon.com/images...   \n",
       "26767  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                                   title formatted_price  \n",
       "27547  ❀Nation Women Stripe Blouse Long Sleeve Shirt ...            None  \n",
       "31277  ✽ANGLIN✽ Women Striped Floral Long Sleeve Roun...            None  \n",
       "30453  ✽ANGLIN✽ Women Striped Floral Long Sleeve Roun...            None  \n",
       "32485  ✽ANGLIN✽ Women Fashion Stripe Dress Round Coll...            None  \n",
       "26767  ✽ANGLIN✽ Women Autumn Winter Christmas Printin...            None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sorted.sort_values('title',inplace=True, ascending=False)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some examples of dupliacte titles that differ only in the last few words.\n",
    "<pre>\n",
    "Titles 1:\n",
    "16. woman's place is in the house and the senate shirts for Womens XXL White\n",
    "17. woman's place is in the house and the senate shirts for Womens M Grey\n",
    "\n",
    "Title 2:\n",
    "25. tokidoki The Queen of Diamonds Women's Shirt X-Large\n",
    "26. tokidoki The Queen of Diamonds Women's Shirt Small\n",
    "27. tokidoki The Queen of Diamonds Women's Shirt Large\n",
    "\n",
    "Title 3:\n",
    "61. psychedelic colorful Howling Galaxy Wolf T-shirt/Colorful Rainbow Animal Print Head Shirt for woman Neon Wolf t-shirt\n",
    "62. psychedelic colorful Howling Galaxy Wolf T-shirt/Colorful Rainbow Animal Print Head Shirt for woman Neon Wolf t-shirt\n",
    "63. psychedelic colorful Howling Galaxy Wolf T-shirt/Colorful Rainbow Animal Print Head Shirt for woman Neon Wolf t-shirt\n",
    "64. psychedelic colorful Howling Galaxy Wolf T-shirt/Colorful Rainbow Animal Print Head Shirt for woman Neon Wolf t-shirt\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "stage1_dedupe_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    \n",
    "    previous_i = i\n",
    "\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "\n",
    "    # search for the similar products sequentially \n",
    "    j = i+1\n",
    "    while j < num_data_points:\n",
    "\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'Small']\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "\n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a), len(b))\n",
    "\n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different\n",
    "        # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) > 2: # number of words in which both sensences differ\n",
    "            # if both strings are differ by more than 2 words we include the 1st string index\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "\n",
    "            # if the comaprision between is between num_data_points, num_data_points-1 strings and they differ in more than 2 words we include both\n",
    "            if j == num_data_points-1: stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "\n",
    "            # start searching for similar apperals corresponds 2nd string\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points :  48722\n"
     ]
    }
   ],
   "source": [
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]\n",
    "print('Number of data points : ', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We removed  the dupliactes which differ only at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/48k_apperal_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.2.3] Remove duplicates : Part 2\n",
    "<pre>\n",
    "\n",
    "In the previous cell, we sorted whole data in alphabetical order of  titles.Then, we removed titles which are adjacent and very similar title\n",
    "\n",
    "But there are some products whose titles are not adjacent but very similar.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Titles-1\n",
    "86261.  UltraClub Women's Classic Wrinkle-Free Long Sleeve Oxford Shirt, Pink, XX-Large\n",
    "115042. UltraClub Ladies Classic Wrinkle-Free Long-Sleeve Oxford Light Blue XXL\n",
    "\n",
    "TItles-2\n",
    "75004.  EVALY Women's Cool University Of UTAH 3/4 Sleeve Raglan Tee\n",
    "109225. EVALY Women's Unique University Of UTAH 3/4 Sleeve Raglan Tees\n",
    "120832. EVALY Women's New University Of UTAH 3/4-Sleeve Raglan Tshirt\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of stop words: {'having', 'had', 'being', 'don', \"you'd\", 'what', 'same', \"weren't\", 'hadn', 'they', 'up', 'off', 'your', 'yourself', \"it's\", 'y', 'when', 'other', 'isn', 'all', 'won', \"shouldn't\", 'each', 'yours', 'how', 'ours', 'again', 'been', \"haven't\", 'own', \"you've\", 'mustn', 'it', 'not', \"hadn't\", 'a', 'shouldn', 'wasn', 'the', 'did', 'further', 'than', 'in', 'for', 'or', 'he', \"wouldn't\", 'couldn', \"couldn't\", \"you're\", 'so', 'ma', 'only', 'until', 'you', 'was', \"mustn't\", \"don't\", 'is', 'my', 'hers', 'are', \"needn't\", 'once', 'this', 'doing', 'any', 'then', 'his', 'no', \"wasn't\", \"doesn't\", 'wouldn', 'just', 'an', 'hasn', \"you'll\", 'their', 'above', 'both', 'of', 'some', 'now', 'myself', 'll', 'needn', 'haven', \"aren't\", 'doesn', 'am', 'nor', 'can', \"mightn't\", 'weren', 'very', 'few', 'if', 'be', 'such', 'does', 'between', 'do', 'about', 'yourselves', 'these', 'out', 'after', 'which', 'as', 'down', 'and', 'aren', 'whom', 'most', 'has', 'to', 'at', 'there', 'him', 'with', \"hasn't\", 'me', 'themselves', 'theirs', 'our', 'its', 'should', 'o', 'we', \"isn't\", \"that'll\", 'that', 'ourselves', 'those', 'over', 'them', 'through', 'but', 'under', 'while', 'during', 'before', \"should've\", 'into', 'ain', 'on', 'where', 'too', 'below', 'will', 'herself', 'more', 'from', 've', \"won't\", 'didn', 'have', 'she', 's', 'd', 'were', 'i', \"shan't\", 'because', 'by', 'itself', 'against', 'her', 'm', 're', 't', 'shan', 'who', 'himself', \"she's\", \"didn't\", 'here', 'why', 'mightn'}\n",
      "48722\n"
     ]
    }
   ],
   "source": [
    "Total_words=0\n",
    "word_index_table={}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print ('list of stop words:', stop_words)\n",
    "indices = []\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing(total_text, index, column,table):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            \n",
    "            # stop-word removal\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "                # creating word-Index Dictinary\n",
    "                if word in table:\n",
    "                    table[word].append(index)\n",
    "                else:\n",
    "                    table[word]=[index]\n",
    "        data[column][index] = string\n",
    "    return len(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_str(total_text):\n",
    "    if type(total_text) is not int:\n",
    "        string=[]\n",
    "        for words in total_text.split():\n",
    "            # Removing punctuations\n",
    "            word =(\"\".join(e for e in words if e.isalnum()))\n",
    "            # Stop-Word removal\n",
    "            if not word in stop_words:\n",
    "                string.append(word)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrate_index_lis(stringArray):\n",
    "    lis={}\n",
    "    for word in stringArray:\n",
    "        for ele in word_index_table[str(word)]:\n",
    "            if ele in lis:\n",
    "                lis[ele]+=1\n",
    "            else:\n",
    "                lis[ele]=1\n",
    "        #lis=lis+table[str(word)]\n",
    "    return [ele for ele in lis if lis[ele]>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470720\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    words=nlp_preprocessing(row['title'], index, 'title',word_index_table)\n",
    "    Total_words=Total_words+words\n",
    "print(Total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(A, B):\n",
    "    # count\n",
    "    count = {}\n",
    "    # insert A in table\n",
    "    for word in A:\n",
    "        count[word] = count.get(word, 0) + 1\n",
    "    # insert B in table\n",
    "    for word in B:\n",
    "        count[word] = count.get(word, 0) + 1\n",
    "    # return count of Common words\n",
    "    return len([word for word in count if count[word] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar=pyprind.ProgBar(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48722\n"
     ]
    }
   ],
   "source": [
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [#########################     ] 100% | ETA: 00:21:21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 47min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This code snippet takes significant amount of time.\n",
    "# O(n^2) time.\n",
    "# Takes about a 2 hour to run on a decent computer.\n",
    "\n",
    "stage2_dedupe_asins = []\n",
    "while len(indices)!=0:\n",
    "    i = indices.pop()\n",
    "    #print(\"current i \",i)\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    # consider the first apperal's title\n",
    "    a = data['title'].loc[i]\n",
    "    a = preprocess_str(a)\n",
    "    index_list = genrate_index_lis(a)\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    for j in index_list:\n",
    "        #print(\"Current j \",j)\n",
    "        b = data['title'].loc[j]\n",
    "        b = preprocess_str(b)\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "        \n",
    "        length = max(len(a),len(b))\n",
    "        \n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = find(a,b) \n",
    "\n",
    "        # if the number of words in which both strings differ are < 3 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) < 3 and (j in indices) :\n",
    "            indices.remove(j)\n",
    "            #print(\" Removed index \",j)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from whole previous products we will consider only\n",
    "# the products that are found in previous cell\n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points after stage two of dedupe:  41012\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points after stage two of dedupe: ',data.shape[0])\n",
    "# from 48k apperals we reduced to 41k apperals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/41k_apperal_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Already removed puncuations and stop-word removal\n",
    "\n",
    "Now Lowering the text and Word Lemmantization|Steming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"pickels/41k_apperal_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing2(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            words = words.lower()\n",
    "            string =string+\" \"+lmtzr.lemmatize(words)\n",
    "        data[column][index] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing2(row['title'], index, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickels/41k_apperal_data2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    url = row['medium_image_url']\n",
    "    #print(url)\n",
    "    response = requests.get(url)\n",
    "    file = open('images/41k_images/'+row['asin']+'.jpeg','wb')\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "    #img = Image.open(BytesIO(response.content))\n",
    "    #img.save('images/41k_images/'+row['asin']+'.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of some datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('pickels/41k_apperal_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_array=np.load('41k_image_asins.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(asin_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sort_values(by=['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickels/41k_apperal_data3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
