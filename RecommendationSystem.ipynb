{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import itertools\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "#Deep-Learning Library\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "'''import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)'''\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have give a json file which consists of all information about\n",
    "# the products\n",
    "# loading the data using pandas' read_json file.\n",
    "data = pd.read_json('tops_fashion.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of Data Points :\", data.shape[0])\n",
    "print (\"Number of Features /Variables:\" ,data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these 19 features, we will be using only 6 features in this project.\n",
    "    1. asin  ( Amazon standard identification number)\n",
    "    2. brand ( brand to which the product belongs to )\n",
    "    3. color ( Color information of apparel, it can contain many colors as   a value ex: red and black stripes ) \n",
    "    4. product_type_name (type of the apperal, ex: SHIRT/TSHIRT )\n",
    "    5. medium_image_url  ( url of the image )\n",
    "    6. title (title of the product.)\n",
    "    7. formatted_price (price of the product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]\n",
    "print (\"Number of Data Points :\", data.shape[0])\n",
    "print (\"Number of Features /Variables:\" ,data.shape[1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [2]Missing data values for various features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the basic stats for each various feature for Data cleaning and storing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [2.1]Basic stats for the feature: product_type_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['product_type_name'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of different product types\n",
    "print(data['product_type_name'].unique())\n",
    "# find the 10 most frequent product_type_names.\n",
    "product_type_count = Counter(list(data['product_type_name']))\n",
    "product_type_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2.2]Basic stats for the feature: brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['brand'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that there are only 10577 unique brands and total missing brands are 183138-182987=151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_count = Counter(list(data['brand']))\n",
    "brand_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thge Most common Brand is Zago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [2.3]Basic stats for the feature: color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['color'].describe())\n",
    "color_count = Counter(list(data['color']))\n",
    "color_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see 7380 are unique product.\n",
    "###### 64956 of 183138 products have color information. That's approx 35.4%.\n",
    "###### 7.2% of products are black in color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2.4] Basic stats for the feature: formatted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['formatted_price'].describe())\n",
    "price_count = Counter(list(data['formatted_price']))\n",
    "price_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Only 28,395 (15.5% of whole data) products with price information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [2.5]Basic stats for the feature : title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['title'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/180k_apparel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To reduce this large data we will take formatted price such that our null entries are removed for better data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now taking color in it for removing null in the color section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/28k_apparel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have reduced the to 28K because i want to understand some basics operation on small data first so 28k is enough for understanding it .For those who want to work on the 180K data can work on it.I have commented out the code for image 28K data so those who are usiung this can download it from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "for index, row in images.iterrows():\n",
    "        url = row['large_image_url']\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img.save('images/28k_images/'+row['asin']+'.jpeg')\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Removing the duplicated Items by Title name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3.1] Understanding the Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickels/28k_apparel_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(data.duplicated('title')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 2325 products which have same title but different color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3.2] Remove Duplicates : Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickels/28k_apparel_data')\n",
    "data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove All products with very few words in title\n",
    "data_sorted = data[data['title'].apply(lambda x: len(x.split())>4)]\n",
    "print(\"After removal of products with short description:\", data_sorted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted.sort_values('title',inplace=True, ascending=False)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some examples of dupliacte titles that differ only in the last few words.\n",
    "\n",
    "Titles 1:\n",
    "16. woman's place is in the house and the senate shirts for Womens XXL White\n",
    "17. woman's place is in the house and the senate shirts for Womens M Grey\n",
    "\n",
    "Title 2:\n",
    "25. tokidoki The Queen of Diamonds Women's Shirt X-Large\n",
    "26. tokidoki The Queen of Diamonds Women's Shirt Small\n",
    "27. tokidoki The Queen of Diamonds Women's Shirt Large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)\n",
    "stage1_dedupe_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    \n",
    "    previous_i = i\n",
    "\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "\n",
    "    # search for the similar products sequentially \n",
    "    j = i+1\n",
    "    while j < num_data_points:\n",
    "\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'Small']\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "\n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a), len(b))\n",
    "\n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different\n",
    "        # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) > 2: # number of words in which both sensences differ\n",
    "            # if both strings are differ by more than 2 words we include the 1st string index\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "\n",
    "            # if the comaprision between is between num_data_points, num_data_points-1 strings and they differ in more than 2 words we include both\n",
    "            if j == num_data_points-1: stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "\n",
    "            # start searching for similar apperals corresponds 2nd string\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]\n",
    "print('Number of data points : ', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/17k_apperal_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some examples of dupliacte titles that differ only in the between few words.\n",
    "\n",
    "\n",
    "In the previous cell, we sorted whole data in alphabetical order of  titles.Then, we removed titles which are adjacent and very similar title\n",
    "\n",
    "But there are some products whose titles are not adjacent but very similar.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Titles-1\n",
    "86261.  UltraClub Women's Classic Wrinkle-Free Long Sleeve Oxford Shirt, Pink, XX-Large\n",
    "115042. UltraClub Ladies Classic Wrinkle-Free Long-Sleeve Oxford Light Blue XXL\n",
    "\n",
    "TItles-2\n",
    "75004.  EVALY Women's Cool University Of UTAH 3/4 Sleeve Raglan Tee\n",
    "109225. EVALY Women's Unique University Of UTAH 3/4 Sleeve Raglan Tees\n",
    "120832. EVALY Women's New University Of UTAH 3/4-Sleeve Raglan Tshirt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Method below mentioned is brute-force attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('pickels/17k_apperal_data')\n",
    "# O(n^2) time.\n",
    "# Takes about an hour to run on a decent computer.\n",
    "\n",
    "indices = []\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "\n",
    "stage2_dedupe_asins = []\n",
    "while len(indices)!=0:\n",
    "    i = indices.pop()\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    # consider the first apperal's title\n",
    "    a = data['title'].loc[i].split()\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    for j in indices:\n",
    "        \n",
    "        b = data['title'].loc[j].split()\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "        \n",
    "        length = max(len(a),len(b))\n",
    "        \n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are < 3 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) < 3:\n",
    "            indices.remove(j)\n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]\n",
    "print('Number of data points after stage two of dedupe: ',data.shape[0])\n",
    "data.to_pickle('pickels/16k_apperal_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] Data Pre processing using Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickels/16k_apperal_data')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print ('list of stop words:', stop_words)\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            # Conver all letters to lower-case\n",
    "            word = word.lower()\n",
    "            # stop-word removal\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        data[column][index] = string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "# we take each title and we text-preprocess it.\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing(row['title'], index, 'title')\n",
    "# we print the time it took to preprocess whole titles \n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('pickels/16k_apperal_data_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vectorizer = CountVectorizer()\n",
    "title_features   = title_vectorizer.fit_transform(data['title'])\n",
    "title_features.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As in the above we can see that 16435 are titles and 11157 are the word in document corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [5] Creating the IDF weighted Word2Vec Title ,Brand and Color using One Hot Encoder and Visual by VGG16 CovNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5.1] Defining The Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.1.1] Idf Title Features and W2V Title Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "idf_title_vectorizer = CountVectorizer()\n",
    "idf_title_features = idf_title_vectorizer.fit_transform(data['title'])\n",
    "\n",
    "# idf_title_features.shape = #data_points * #words_in_corpus\n",
    "# CountVectorizer().fit_transform(courpus) returns the a sparase matrix of dimensions #data_points * #words_in_corpus\n",
    "# idf_title_features[doc_id, index_of_word_in_corpus] = number of times the word occured in that doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [5.1.2] Defining the Features for Brands and Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the brand values are empty. \n",
    "# Need to replace Null with string \"NULL\"\n",
    "data['brand'].fillna(value=\"Not given\", inplace=True )\n",
    "\n",
    "# replace spaces with hypen\n",
    "brands = [x.replace(\" \", \"-\") for x in data['brand'].values]\n",
    "\n",
    "colors = [x.replace(\" \", \"-\") for x in data['color'].values]\n",
    "\n",
    "#One-Hot Encoding using Count vectorizer\n",
    "brand_vectorizer = CountVectorizer()\n",
    "brand_features = brand_vectorizer.fit_transform(brands)\n",
    "\n",
    "color_vectorizer = CountVectorizer()\n",
    "color_features = color_vectorizer.fit_transform(colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5.2] Utility Functions for the IDF Word2Vec Weighted Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word2vec_model', 'rb') as handle:\n",
    "    model = pickle.load(handle)\n",
    "# vocab = stores all the words that are there in google w2v model\n",
    "vocab = model.keys()\n",
    "doc_id = 0\n",
    "w2v_title_weight = []\n",
    "# for every title we build a weighted vector representation\n",
    "for i in data['title']:\n",
    "    w2v_title_weight.append(build_avg_vec(i, 300, doc_id,'weighted'))\n",
    "    doc_id += 1\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title_weight = np.array(w2v_title_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_containing(word):\n",
    "    # return the number of documents which had the given word\n",
    "    return sum(1 for blob in data['title'] if word in blob.split())\n",
    "\n",
    "def idf(word):\n",
    "    # idf = log(#number of docs / #number of docs which had the given word)\n",
    "    return math.log(data.shape[0] / (n_containing(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to convert the values into float\n",
    "idf_title_features  = idf_title_features.astype(np.float)\n",
    "\n",
    "for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "    # for every word in whole corpus we will find its idf value\n",
    "    idf_val = idf(i)\n",
    "    \n",
    "    # to calculate idf_title_features we need to replace the count values with the idf values of the word\n",
    "    # idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0] will return all documents in which the word i present\n",
    "    for j in idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "        \n",
    "        # we replace the count values of word i in document j with  idf_value of word i \n",
    "        # idf_title_features[doc_id, index_of_word_in_courpus] = idf value of word\n",
    "        idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vec(sentence, doc_id, m_name):\n",
    "    # sentence : title of the apparel\n",
    "    # doc_id: document id in our corpus\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if m_name == 'weighted' and i in  idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])\n",
    "            elif m_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "        else:\n",
    "            # if the word in our courpus is not there in the google word2vec corpus, we are just ignoring it\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    # we will return a numpy array of shape (#number of words in title * 300 ) 300 = len(w2v_model[word])\n",
    "    # each row represents the word2vec representation of each word (weighted/avg) in given sentance \n",
    "    return  np.array(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance(vec1, vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    \n",
    "    final_dist = []\n",
    "    # for each vector in vec1 we caluclate the distance(euclidean) to all vectors in vec2\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            # np.linalg.norm(i-j) will result the euclidean distance between vectors i, j\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "    # final_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # final_dist[i,j] = euclidean distance between vectors i, j\n",
    "    return np.array(final_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# this function will add the vectors of each word and returns the avg vector of given sentance\n",
    "def build_avg_vec(sentence, num_features, doc_id, m_name):\n",
    "    # sentace: its title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        nwords += 1\n",
    "        if word in vocab:\n",
    "            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])\n",
    "            elif m_name == 'avg':\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # returns the avg vector of given sentance, its of shape (1, 300)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it in notebook \n",
    "    plt.imshow(img)\n",
    "#The Below code is is for the weighted IDF Word2Vec \n",
    "def heat_map_w2v(sentence1, sentence2, url, doc_id1, doc_id2, model):\n",
    "    # sentance1 : title1, input apparel\n",
    "    # sentance2 : title2, recommended apparel\n",
    "    # url: apparel image url\n",
    "    # doc_id1: document id of input apparel\n",
    "    # doc_id2: document id of recommended apparel\n",
    "    # model: it can have two values, 1. avg 2. weighted\n",
    "    \n",
    "    #s1_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s1_vec = get_word_vec(sentence1, doc_id1, model)\n",
    "    #s2_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s2_vec = get_word_vec(sentence2, doc_id2, model)\n",
    "\n",
    "    # s1_s2_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # s1_s2_dist[i,j] = euclidean distance between words i, j\n",
    "    s1_s2_dist = get_distance(s1_vec, s2_vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of apparel\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4,1],height_ratios=[2,1]) \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    # ploting the heap map based on the pairwise distances\n",
    "    ax = sns.heatmap(np.round(s1_s2_dist,4), annot=True)\n",
    "    # set the x axis labels as recommended apparels title\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    ax = plt.subplot(gs[1])\n",
    "    # we remove all grids and axis labels for image\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    display_img(url, ax, fig)\n",
    "    \n",
    "    plt.show()\n",
    "def weighted_w2v_model(doc_id, num_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    \n",
    "    # pairwise_dist will store the distance from given input apparel to all remaining apparels\n",
    "    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "    # http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "    pairwise_dist = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0, len(indices)):\n",
    "        heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'weighted')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('Brand :',data['brand'].loc[df_indices[i]])\n",
    "        print('euclidean distance from input :', pdists[i])\n",
    "        print('='*125)\n",
    "weighted_w2v_model(14443,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [5.3] Creating VGG16 ConvNet using Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
